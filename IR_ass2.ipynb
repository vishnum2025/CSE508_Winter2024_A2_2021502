{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTik5tZqKcca",
        "outputId": "b860d31c-7395-4af1-aae6-3fb7b839227e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset_path = '/content/drive/My Drive/IR/A2_Data.csv'\n",
        "df = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "mzEdua8dIQ6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Image Feature Extraction"
      ],
      "metadata": {
        "id": "SVAUyNKs8Z_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision import models, transforms\n",
        "import requests\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "from collections import defaultdict, Counter\n",
        "from math import log\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "output_dir = '/content/drive/My Drive/IR'\n",
        "\n",
        "# Define image preprocessing\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to extract features from an image\n",
        "def extract_image_features(url):\n",
        "    try:\n",
        "        # Check if the URL is in a list format as a string, and convert if necessary\n",
        "        import ast\n",
        "        if url.startswith(\"[\") and url.endswith(\"]\"):\n",
        "            url = ast.literal_eval(url)[0]  # Safely evaluates the string as a list and gets the first element\n",
        "\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        img_t = image_transforms(img)\n",
        "        img_t = img_t.unsqueeze(0)  # Add batch dimension\n",
        "        with torch.no_grad():\n",
        "            features = resnet(img_t)\n",
        "        return features.cpu().numpy().flatten()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"RequestException for URL {url}: {e}\")\n",
        "    except UnidentifiedImageError:  # Use the exception directly without the 'PIL.' prefix\n",
        "        print(f\"UnidentifiedImageError: cannot identify image file from URL {url}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error for URL {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Extract features\n",
        "image_features = []\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    image_feature = extract_image_features(row['Image'])\n",
        "    if image_feature is not None:\n",
        "        image_features.append(image_feature)\n",
        "\n",
        "# Save the results\n",
        "with open(os.path.join(output_dir, 'image_features.pkl'), 'wb') as f:\n",
        "    pickle.dump(image_features, f)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxOG1gXHEtkO",
        "outputId": "42a4705e-91ed-451a-e972-e9338b3a6b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 64.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/71F3npeHUDL._SY88.jpg.\n",
            "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/71B8OOE5N8L._SY88.jpg.\n",
            "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/718niQ1GEwL._SY88.jpg.\n",
            "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/61OboZT-kcL._SY88.jpg.\n",
            "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/710a2Pyh5lL._SY88.jpg.\n",
            "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/816NMd0LexL._SY88.jpg.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Text Feature Extraction"
      ],
      "metadata": {
        "id": "3JGw-6MC8WDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase conversion\n",
        "    text = text.lower()\n",
        "    # Remove URLs, hashtags, and mentions\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Stopwords removal, stemming, and lemmatization\n",
        "    tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens if not word in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Initialize NLP tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "text_data = df['Review Text'].fillna('').tolist()\n",
        "\n",
        "# Preprocess text data\n",
        "tokenized_texts = [preprocess_text(text) for text in text_data]\n",
        "\n",
        "# Manual TF-IDF Calculation\n",
        "def compute_tf_idf(tokenized_docs):\n",
        "    # Calculate TF (term frequency)\n",
        "    tf = [{word: doc.count(word) / len(doc) for word in doc} for doc in tokenized_docs]\n",
        "\n",
        "    # Calculate document frequency (DF)\n",
        "    df = {}\n",
        "    for doc in tokenized_docs:\n",
        "        for word in set(doc):\n",
        "            df[word] = df.get(word, 0) + 1\n",
        "\n",
        "    # Calculate IDF (inverse document frequency)\n",
        "    idf = {word: math.log(len(tokenized_docs) / freq) for word, freq in df.items()}\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tf_idf = [{word: freq * idf[word] for word, freq in doc.items()} for doc in tf]\n",
        "    return tf_idf\n",
        "\n",
        "tf_idf_scores = compute_tf_idf(tokenized_texts)\n",
        "\n",
        "# Save the tokenized texts and TF-IDF scores using pickle\n",
        "output_dir = '/content/drive/My Drive/IR'  # Ensure this directory exists\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "tokenized_texts_path = os.path.join(output_dir, 'tokenized_texts.pkl')\n",
        "tf_idf_scores_path = os.path.join(output_dir, 'tf_idf_scores_manual_text.pkl')\n",
        "\n",
        "with open(tokenized_texts_path, 'wb') as f:\n",
        "    pickle.dump(tokenized_texts, f)\n",
        "\n",
        "with open(tf_idf_scores_path, 'wb') as f:\n",
        "    pickle.dump(tf_idf_scores, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXNT2Z8vRyXF",
        "outputId": "36cfd57d-36a9-48c8-9ed4-872b241f46d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Image Retrieval and Text Retrieval"
      ],
      "metadata": {
        "id": "oVq1vWNV8RIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import requests\n",
        "from PIL import Image, ImageEnhance\n",
        "from io import BytesIO\n",
        "from scipy.spatial.distance import cosine\n",
        "import os\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import requests\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "from collections import defaultdict, Counter\n",
        "from math import log\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Define the path to your dataset and precomputed features\n",
        "dataset_path = '/content/drive/My Drive/IR/A2_Data.csv'\n",
        "output_dir = '/content/drive/My Drive/IR'\n",
        "image_features_path = os.path.join(output_dir, 'image_features.pkl')\n",
        "tf_idf_scores_path = os.path.join(output_dir, 'tf_idf_scores_manual_text.pkl')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Load precomputed image features and TF-IDF scores\n",
        "with open(image_features_path, 'rb') as f:\n",
        "    image_features = pickle.load(f)\n",
        "with open(tf_idf_scores_path, 'rb') as f:\n",
        "    tf_idf_scores = pickle.load(f)\n",
        "\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "resnet_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define image transformations\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "\n",
        "  if isinstance(v1, np.ndarray) and all(isinstance(v, np.ndarray) for v in v2):\n",
        "          # Convert list of numpy arrays (v2) to a single 2D numpy array\n",
        "          v2 = np.array(v2)\n",
        "          # Normalize v1 and v2\n",
        "          v1_norm = v1 / np.linalg.norm(v1)\n",
        "          v2_norm = v2 / np.linalg.norm(v2, axis=1)[:, np.newaxis]\n",
        "          # Calculate cosine similarity\n",
        "          similarities = np.dot(v1_norm, v2_norm.T)\n",
        "\n",
        "      # Case for sparse vectors (TF-IDF scores)\n",
        "  elif isinstance(v1, dict) and all(isinstance(v, dict) for v in v2):\n",
        "          similarities = []\n",
        "          for tfidf_dict in v2:\n",
        "              # Intersection of keys (terms present in both vectors)\n",
        "              common_terms = set(v1.keys()) & set(tfidf_dict.keys())\n",
        "              # Manual dot product for common terms\n",
        "              dot_product = sum(v1[term] * tfidf_dict[term] for term in common_terms)\n",
        "              # Norms of the vectors\n",
        "              norm_v1 = np.sqrt(sum(value ** 2 for value in v1.values()))\n",
        "              norm_v2 = np.sqrt(sum(value ** 2 for value in tfidf_dict.values()))\n",
        "              # Cosine similarity\n",
        "              if norm_v1 == 0 or norm_v2 == 0:\n",
        "                  similarity = 0\n",
        "              else:\n",
        "                  similarity = dot_product / (norm_v1 * norm_v2)\n",
        "              similarities.append(similarity)\n",
        "          similarities = np.array(similarities)\n",
        "\n",
        "  else:\n",
        "          raise ValueError(\"Unsupported input types.\")\n",
        "\n",
        "  return similarities\n",
        "\n",
        "\n",
        "\n",
        "def find_most_similar_images(processed_image, precomputed_features, top_n=3):\n",
        "    # Assuming direct comparison of processed_image array to precomputed feature vectors\n",
        "    similarities = cosine_similarity(processed_image, precomputed_features).flatten()\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    return top_indices, [similarities[i] for i in top_indices]\n",
        "\n",
        "def find_most_similar_reviews(input_tfidf, precomputed_tfidf_scores, top_n=3):\n",
        "    # Calculate cosine similarity between the input TF-IDF vector and each precomputed TF-IDF vector\n",
        "    similarities = cosine_similarity(input_tfidf, precomputed_tfidf_scores).flatten()\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    top_similarities = [similarities[i] for i in top_indices]\n",
        "    return top_indices, top_similarities\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing\"\"\"\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = text.split()  # Tokenize by splitting on whitespace\n",
        "    # Optionally remove stopwords here\n",
        "    return tokens\n",
        "def compute_tf(tokenized_review):\n",
        "    tf = {}\n",
        "    for word in tokenized_review:\n",
        "        tf[word] = tf.get(word, 0) + 1\n",
        "\n",
        "    # Normalize term frequencies by the total number of words in the document\n",
        "    total_words = len(tokenized_review)\n",
        "    tf = {word: count / total_words for word, count in tf.items()}\n",
        "\n",
        "    return tf\n",
        "\n",
        "def preprocess_image(image_url):\n",
        "    \"\"\"Fetch and preprocess an image from a URL, then extract features.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        # Apply preprocessing transformations\n",
        "        processed_image = transform_pipeline(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Extract features with the model\n",
        "        with torch.no_grad():\n",
        "            features = resnet_model(processed_image)\n",
        "\n",
        "        # Convert features to a numpy array\n",
        "        features_np = features.numpy().flatten()\n",
        "        return features_np\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image from URL {image_url}: {e}\")\n",
        "        return None\n",
        "## INPUT ##\n",
        "input_image_url = input(\"Enter the image URL: \")\n",
        "input_review_text = input(\"Enter the review text: \")\n",
        "\n",
        "\n",
        "# Preprocess review text\n",
        "processed_tokens = preprocess_text(input_review_text)\n",
        "input_review_tfidf = compute_tf(processed_tokens)\n",
        "\n",
        "# Preprocess the image from URL\n",
        "processed_image = preprocess_image(input_image_url)\n",
        "\n",
        "if processed_image is not None:\n",
        "\n",
        "    # Find the most similar images and reviews\n",
        "    similar_image_indices, image_similarities = find_most_similar_images(processed_image, image_features)\n",
        "    similar_review_indices, review_similarities = find_most_similar_reviews(input_review_tfidf, tf_idf_scores)\n",
        "\n",
        "    print(\"Similar Image Indices:\", similar_image_indices)\n",
        "    print(\"Image Similarities:\", image_similarities)\n",
        "    print(\"Similar Review Indices:\", similar_review_indices)\n",
        "    print(\"Review Similarities:\", review_similarities)\n",
        "else:\n",
        "    print(\"The specified image URL and review were not found in the dataset, or image processing failed.\")\n",
        "\n",
        "# Save the retrieval results\n",
        "retrieval_results = {\n",
        "    'similar_image_indices': similar_image_indices,\n",
        "    'image_similarities': image_similarities,\n",
        "    'similar_review_indices': similar_review_indices,\n",
        "    'review_similarities': review_similarities,\n",
        "}\n",
        "results_path = os.path.join(output_dir, 'retrieval_results.pkl')\n",
        "with open(results_path, 'wb') as f:\n",
        "    pickle.dump(retrieval_results, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPn_WcHPNooq",
        "outputId": "4756feff-536c-4c27-c499-2e0038a08805"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 124MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the image URL: https://images-na.ssl-images-amazon.com/images/I/81q5+IxFVUL._SY88.jpg\n",
            "Enter the review text: Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go.\n",
            "Similar Image Indices: [  0  62 193]\n",
            "Image Similarities: [0.8524184, 0.836084, 0.8211998]\n",
            "Similar Review Indices: [  0 794 750]\n",
            "Review Similarities: [0.14389762284190002, 0.14002800840280097, 0.1264111506377715]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Combined Retrieval"
      ],
      "metadata": {
        "id": "M-JSBvldg_QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.spatial.distance import cdist\n",
        "import os\n",
        "import pandas as pd\n",
        "output_dir = '/content/drive/My Drive/IR'\n",
        "dataset_path = '/content/drive/My Drive/IR/A2_Data.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Assuming output_dir is already defined\n",
        "results_path = os.path.join(output_dir, 'retrieval_results.pkl')\n",
        "\n",
        "with open(results_path, 'rb') as f:\n",
        "    retrieval_results = pickle.load(f)\n",
        "\n",
        "# Extract individual components from the loaded retrieval results\n",
        "similar_image_indices = retrieval_results['similar_image_indices']\n",
        "image_similarities = retrieval_results['image_similarities']\n",
        "similar_review_indices = retrieval_results['similar_review_indices']\n",
        "review_similarities = retrieval_results['review_similarities']\n",
        "\n",
        "\n",
        "def calculate_composite_scores(image_similarities, review_similarities):\n",
        "    composite_scores = []\n",
        "    for image_similarity, review_similarity in zip(image_similarities, review_similarities):\n",
        "        # Calculate the average similarity score for each pair\n",
        "        composite_score = (image_similarity + review_similarity) / 2\n",
        "        composite_scores.append(composite_score)\n",
        "    return composite_scores\n",
        "\n",
        "composite_scores = calculate_composite_scores(image_similarities, review_similarities)\n",
        "\n",
        "# Create a list of tuples (composite_score, image_index, review_index) and sort it\n",
        "ranked_pairs = sorted(zip(composite_scores, similar_image_indices, similar_review_indices), reverse=True, key=lambda x: x[0])\n",
        "\n",
        "# Display the ranked results\n",
        "print(\"Ranked Combined Retrieval Results:\")\n",
        "for rank, (comp_score, img_idx, rev_idx) in enumerate(ranked_pairs, start=1):\n",
        "    print(f\"Rank: {rank}, Image Index: {img_idx}, Review Index: {rev_idx}, Composite Score: {comp_score:.4f}\")\n",
        "\n",
        "\n",
        "ranked_results_path = os.path.join(output_dir, 'ranked_combined_retrieval_results.pkl')\n",
        "with open(ranked_results_path, 'wb') as f:\n",
        "    pickle.dump(ranked_pairs, f)\n",
        "\n",
        "print(f\"Ranked combined retrieval results saved to: {ranked_results_path} \\n\")\n",
        "\n",
        "\n",
        "def get_data_by_indices(df, image_indices, review_indices):\n",
        "    # Extract image URLs and reviews by indices\n",
        "    image_urls = df.loc[image_indices, 'Image'].tolist()\n",
        "    reviews = df.loc[review_indices, 'Review Text'].tolist()\n",
        "    return image_urls, reviews\n",
        "\n",
        "\n",
        "image_urls, reviews = get_data_by_indices(df, similar_image_indices, similar_review_indices)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU6GWkvnFy9L",
        "outputId": "86f06b2a-2009-4fae-bfa4-15ea8015ea56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Combined Retrieval Results:\n",
            "Rank: 1, Image Index: 0, Review Index: 0, Composite Score: 0.4982\n",
            "Rank: 2, Image Index: 62, Review Index: 794, Composite Score: 0.4881\n",
            "Rank: 3, Image Index: 193, Review Index: 750, Composite Score: 0.4738\n",
            "Ranked combined retrieval results saved to: /content/drive/My Drive/IR/ranked_combined_retrieval_results.pkl \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Results and Analysis"
      ],
      "metadata": {
        "id": "KjbNgUZv4GXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/drive/My Drive/IR'\n",
        "import pandas as pd\n",
        "dataset_path = '/content/drive/My Drive/IR/A2_Data.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "def calculate_composite_scores(image_similarities, review_similarities):\n",
        "    composite_scores = []\n",
        "    # Assuming image_similarities and review_similarities are aligned and of equal length\n",
        "    for i in range(len(image_similarities)):\n",
        "        comp_score = (image_similarities[i] + review_similarities[i]) / 2\n",
        "        composite_scores.append((i, i, comp_score))  # Use i for both indices, or adjust as needed\n",
        "    composite_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "    return composite_scores\n",
        "\n",
        "\n",
        "\n",
        "## Calculate composite scores using only similarity scores\n",
        "composite_scores = calculate_composite_scores(image_similarities, review_similarities)\n",
        "\n",
        "## Display the combined retrieval results\n",
        "print(\"USING IMAGE RETRIEVAL\")\n",
        "for i, (img_idx, rev_idx, comp_score) in enumerate(composite_scores, start=1):\n",
        "     # Assuming each index points to relevant data in placeholder lists\n",
        "     print(f\"{i}) Image URL: {image_urls[img_idx]}\")  # Example: Single URL or a list if applicable\n",
        "     print(f\"Review: {reviews[rev_idx]}\")\n",
        "     print(f\"Cosine similarity of images - {image_similarities[img_idx]:.4f}\")\n",
        "     print(f\"Cosine similarity of text - {review_similarities[rev_idx]:.4f}\\n\")\n",
        "\n",
        " # Assuming image_similarities and review_similarities are lists of scores from which composite scores were derived\n",
        "composite_image_score = sum(image_similarities) / len(image_similarities)\n",
        "composite_text_score = sum(review_similarities) / len(review_similarities)\n",
        "final_composite_score = (composite_image_score + composite_text_score) / 2\n",
        "\n",
        "print(\"Composite similarity scores of images:\", f\"{composite_image_score:.4f}\")\n",
        "print(\"Composite similarity scores of text:\", f\"{composite_text_score:.4f}\")\n",
        "print(\"Final composite similarity score:\", f\"{final_composite_score:.4f}\\n\")\n",
        "\n",
        "\n",
        "print(\"USING TEXT RETRIEVAL\")\n",
        "for i, (img_idx, rev_idx, comp_score) in enumerate(composite_scores, start=1):\n",
        "     # Assuming 'image_urls[img_idx]' fetches URLs of similar images based on text query\n",
        "     # and 'reviews[rev_idx]' fetches the corresponding review\n",
        "     print(f\"{i}) Image URL: {image_urls[img_idx]}\")  # List of Image URLs could be just this one for simplicity\n",
        "     print(f\"Review: {reviews[rev_idx]}\")  # Extracted Review\n",
        "     print(f\"Cosine similarity of images - {image_similarities[img_idx]:.4f}\")\n",
        "     print(f\"Cosine similarity of text - {review_similarities[rev_idx]:.4f}\\n\")\n",
        "\n",
        "# # To compute final composite scores across all images and reviews:\n",
        "final_composite_image_score = sum(image_similarities) / len(image_similarities)\n",
        "final_composite_text_score = sum(review_similarities) / len(review_similarities)\n",
        "final_composite_score = (final_composite_image_score + final_composite_text_score) / 2\n",
        "\n",
        "print(\"Composite similarity scores of images:\", final_composite_image_score)\n",
        "print(\"Composite similarity scores of text:\", final_composite_text_score)\n",
        "print(\"Final composite similarity score:\", final_composite_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I8sfap1HEn5",
        "outputId": "8c6d5fc1-5094-459b-b8a6-adfd422656dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING IMAGE RETRIEVAL\n",
            "1) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/81q5+IxFVUL._SY88.jpg']\n",
            "Review: Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go.\n",
            "Cosine similarity of images - 0.8524\n",
            "Cosine similarity of text - 0.1439\n",
            "\n",
            "2) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/71nSUnv7znL._SY88.jpg']\n",
            "Review: Good\n",
            "Cosine similarity of images - 0.8361\n",
            "Cosine similarity of text - 0.1400\n",
            "\n",
            "3) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/81Eq6y34BYL._SY88.jpg']\n",
            "Review: Great Quality, adjustable tension. Well made.\n",
            "Cosine similarity of images - 0.8212\n",
            "Cosine similarity of text - 0.1264\n",
            "\n",
            "Composite similarity scores of images: 0.8366\n",
            "Composite similarity scores of text: 0.1368\n",
            "Final composite similarity score: 0.4867\n",
            "\n",
            "USING TEXT RETRIEVAL\n",
            "1) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/81q5+IxFVUL._SY88.jpg']\n",
            "Review: Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go.\n",
            "Cosine similarity of images - 0.8524\n",
            "Cosine similarity of text - 0.1439\n",
            "\n",
            "2) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/71nSUnv7znL._SY88.jpg']\n",
            "Review: Good\n",
            "Cosine similarity of images - 0.8361\n",
            "Cosine similarity of text - 0.1400\n",
            "\n",
            "3) Image URL: ['https://images-na.ssl-images-amazon.com/images/I/81Eq6y34BYL._SY88.jpg']\n",
            "Review: Great Quality, adjustable tension. Well made.\n",
            "Cosine similarity of images - 0.8212\n",
            "Cosine similarity of text - 0.1264\n",
            "\n",
            "Composite similarity scores of images: 0.8365674018859863\n",
            "Composite similarity scores of text: 0.1367789272941575\n",
            "Final composite similarity score: 0.4866731645900719\n"
          ]
        }
      ]
    }
  ]
}